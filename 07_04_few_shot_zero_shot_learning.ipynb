{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "generator = pipeline('text-generation', model='gpt2', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis\n",
      "Text: I hate it when my phone battery dies.\n",
      "Sentiment: Negative\n",
      "###\n",
      "Text: My day has been really great!\n",
      "Sentiment: Positive\n",
      "###\n",
      "Text: This new music video was so good\n",
      "Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generator('''Sentiment Analysis\n",
    "Text: I hate it when my phone battery dies.\n",
    "Sentiment: Negative\n",
    "###\n",
    "Text: My day has been really great!\n",
    "Sentiment: Positive\n",
    "###\n",
    "Text: This new music video was so good\n",
    "Sentiment:''', top_k=2, temperature=0.1, max_length=55)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 55, but `max_length` is set to 55. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis\n",
      "Text: I hate it when my phone battery dies.\n",
      "Sentiment: Negative\n",
      "###\n",
      "Text: My day has been really great!\n",
      "Sentiment: Positive\n",
      "###\n",
      "Text: Not a fan when it is coludy\n",
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "print(generator('''Sentiment Analysis\n",
    "Text: I hate it when my phone battery dies.\n",
    "Sentiment: Negative\n",
    "###\n",
    "Text: My day has been really great!\n",
    "Sentiment: Positive\n",
    "###\n",
    "Text: Not a fan when it is coludy\n",
    "Sentiment:''', top_k=2, temperature=0.1, max_length=55)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question/Answering\n",
      "C: Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock.\n",
      "Q: When was google founded?\n",
      "A: 1998\n",
      "###\n",
      "C: Hugging Face is a company which develops social AI-run chatbot applications. It was established in 2016 by Clement Delangue and Julien Chaumond. The company is based in Brooklyn, New York, United States.\n",
      "Q: What does Hugging Fase develop?\n",
      "A: social AI-run chatbot applications\n",
      "###\n",
      "C: The New York Jets are a professional American football team based in the New York metropolitan area. The Jets compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) East division.\n",
      "Q: What division do the Jets play in?\n",
      "A: The AFC East\n",
      "###\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "print(generator('''Question/Answering\n",
    "C: Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock.\n",
    "Q: When was google founded?\n",
    "A: 1998\n",
    "###\n",
    "C: Hugging Face is a company which develops social AI-run chatbot applications. It was established in 2016 by Clement Delangue and Julien Chaumond. The company is based in Brooklyn, New York, United States.\n",
    "Q: What does Hugging Fase develop?\n",
    "A: social AI-run chatbot applications\n",
    "###\n",
    "C: The New York Jets are a professional American football team based in the New York metropolitan area. The Jets compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) East division.\n",
    "Q: What division do the Jets play in?\n",
    "A:''', top_k=2, num_beams=2, temperature=0.5, max_length=215)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question/Answering\n",
      "C: The New York Jets are a professional American football team based in the New York metropolitan area. The Jets compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) East division.\n",
      "Q: What division do the Jets play in?\n",
      "A: The Jets play in the AFC East, which is the\n"
     ]
    }
   ],
   "source": [
    "print(generator('''Question/Answering\n",
    "C: The New York Jets are a professional American football team based in the New York metropolitan area. The Jets compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) East division.\n",
    "Q: What division do the Jets play in?\n",
    "A:''', top_k=3, num_beams=2, max_length=80, temperature=0.5)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis\n",
      "Text: This new music video was so good\n",
      "Sentiment: I'm so glad you liked it\n",
      "Sentiment: I'm so glad you liked it\n",
      "Sentiment: I'm so glad you liked it\n",
      "Sentiment: I'm so glad you\n"
     ]
    }
   ],
   "source": [
    "print(generator('''Sentiment Analysis\n",
    "Text: This new music video was so good\n",
    "Sentiment:''', top_k=2, temperature=0.1, max_length=55)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_summarize = '''\n",
    "This training will focus on how the GPT family of models are used for NLP tasks including abstractive text summarization and natural language generation. The training will begin with an introduction to necessary concepts including masked self attention, language models, and transformers and then build on those concepts to introduce the GPT architecture. We will then move into how GPT is used for multiple natural language processing tasks with hands-on examples of using pre-trained GPT-2 models as well as fine-tuning these models on custom corpora. We will also use modern GPT models like ChatGPT and GPT-3 to see how to engineer prompts effectively and efficiently for use in production.\n",
    "\n",
    "GPT models are some of the most relevant NLP architectures today and it is closely related to other important NLP deep learning models like BERT. Both of these models are derived from the newly invented transformer architecture and represent an inflection point in how machines process language and context.\n",
    "\n",
    "The Natural Language Processing with Next-Generation Transformer Architectures series of online trainings provides a comprehensive overview of state-of-the-art natural language processing (NLP) models including GPT and BERT which are derived from the modern attention-driven transformer architecture and the applications these models are used to solve today. All of the trainings in the series blend theory and application through the combination of visual mathematical explanations, straightforward applicable Python examples within hands-on Jupyter notebook demos, and comprehensive case studies featuring modern problems solvable by NLP models. (Note that at any given time, only a subset of these classes will be scheduled and open for registration.)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization Task:\n",
      "\n",
      "This training will focus on how the GPT family of models are used for NLP tasks including abstractive text summarization and natural language generation. The training will begin with an introduction to necessary concepts including masked self attention, language models, and transformers and then build on those concepts to introduce the GPT architecture. We will then move into how GPT is used for multiple natural language processing tasks with hands-on examples of using pre-trained GPT-2 models as well as fine-tuning these models on custom corpora. We will also use modern GPT models like ChatGPT and GPT-3 to see how to engineer prompts effectively and efficiently for use in production.\n",
      "\n",
      "GPT models are some of the most relevant NLP architectures today and it is closely related to other important NLP deep learning models like BERT. Both of these models are derived from the newly invented transformer architecture and represent an inflection point in how machines process language and context.\n",
      "\n",
      "The Natural Language Processing with Next-Generation Transformer Architectures series of online trainings provides a comprehensive overview of state-of-the-art natural language processing (NLP) models including GPT and BERT which are derived from the modern attention-driven transformer architecture and the applications these models are used to solve today. All of the trainings in the series blend theory and application through the combination of visual mathematical explanations, straightforward applicable Python examples within hands-on Jupyter notebook demos, and comprehensive case studies featuring modern problems solvable by NLP models. (Note that at any given time, only a subset of these classes will be scheduled and open for registration.)\n",
      "\n",
      "TL;DR: The GTP model is a powerful and versatile NPL model for deep-learning tasks. It is the first NPP model to be used in a large-scale NSP training program.\n"
     ]
    }
   ],
   "source": [
    "print(generator(\n",
    "    f'''Summarization Task:\\n{to_summarize}\\nTL;DR:''',\n",
    "    max_length=512, top_k=2, temperature=0.5, no_repeat_ngram_size=2\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
