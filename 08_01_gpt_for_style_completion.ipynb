{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, GPT2LMHeadModel, pipeline, \\\n",
    "                          Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_data = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='data/sample.txt',\n",
    "    block_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6154"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  171,   119,   123,   464,  4935, 20336, 46566,    11,   317, 11217,\n",
       "           786,   319, 29015, 18493,    11,   416, 22578,   198, 22362,   372,\n",
       "          1355,   721,   372,   628,   198,  1212, 46566,   318,   329,   262,\n",
       "           779,   286]),\n",
       " torch.Size([32]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[0], text_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook, A Treatise on Domestic Economy, by Catherine\n",
      "Esther Beecher\n",
      "\n",
      "\n",
      "This eBook is for the use of\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(text_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<|endoftext|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) #mlm: Masked Language Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   40,   716,   281,  5128],\n",
       "        [ 2396,   716,   314, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 0]]), 'labels': tensor([[  40,  716,  281, 5128],\n",
       "        [2396,  716,  314, -100]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator_example = data_collator([tokenizer('I am an input'), tokenizer('So am I')])\n",
    "collator_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,   716,   281,  5128],\n",
       "        [ 2396,   716,   314, 50256]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator_example.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator_example.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  40,  716,  281, 5128],\n",
       "        [2396,  716,  314, -100]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator_example.labels # -100 to ignore loss calculation for the padded token\n",
    "                        # labels are shifted inside the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "pretrained_generator = pipeline(\n",
    "    'text-generation', \n",
    "    model=model,\n",
    "    tokenizer='gpt2',\n",
    "    config={'max_new_tokens': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Women's role in the market should be to make women buy better brands, but more so than \"better food â€“ but more people like better food.\"\n",
      "\n",
      "We've seen many examples on this page.\n",
      "\n",
      "We have to change how that works\n",
      "----------\n",
      "Women's role in the market should come at least as important one to people who live in states with anti-discrimination law but still think they can still vote.\n",
      "\n",
      "\"I agree on many things and a lot of people have really bad views about\n",
      "----------\n",
      "Women's role in the market should not matter much if they do not receive adequate incentives to support their own children â€” the state should not have to subsidize their own children. The state should not have to subsidize their own children because there is a\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('-'*10)\n",
    "for generated_sequence in pretrained_generator(\"Women's role in the market should\", num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='gpt2_text',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=len(text_data.examples) // 5,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=text_data.examples[:int(len(text_data.examples)*.8)],\n",
    "    eval_dataset=text_data.examples[int(len(text_data.examples)*.8):],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1231\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.69537353515625,\n",
       " 'eval_runtime': 1.7537,\n",
       " 'eval_samples_per_second': 701.936,\n",
       " 'eval_steps_per_second': 22.238}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 4923\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 770\n",
      "  Number of trainable parameters = 124439808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='309' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [309/770 01:12 < 01:49, 4.23 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.491000</td>\n",
       "      <td>4.003829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/39 00:01 < 00:00, 22.22 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1231\n",
      "  Batch size = 32\n",
      "  Num examples = 1231\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to gpt2_text/checkpoint-154\n",
      "Saving model checkpoint to gpt2_text/checkpoint-154\n",
      "Configuration saved in gpt2_text/checkpoint-154/config.json\n",
      "Configuration saved in gpt2_text/checkpoint-154/generation_config.json\n",
      "Model weights saved in gpt2_text/checkpoint-154/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1231\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1231\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to gpt2_text/checkpoint-308\n",
      "Saving model checkpoint to gpt2_text/checkpoint-308\n",
      "Configuration saved in gpt2_text/checkpoint-308/config.json\n",
      "Configuration saved in gpt2_text/checkpoint-308/generation_config.json\n",
      "Model weights saved in gpt2_text/checkpoint-308/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1231\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to gpt2_text/checkpoint-462\n",
      "Configuration saved in gpt2_text/checkpoint-462/config.json\n",
      "Configuration saved in gpt2_text/checkpoint-462/generation_config.json\n",
      "Model weights saved in gpt2_text/checkpoint-462/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1231\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to gpt2_text/checkpoint-616\n",
      "Configuration saved in gpt2_text/checkpoint-616/config.json\n",
      "Configuration saved in gpt2_text/checkpoint-616/generation_config.json\n",
      "Model weights saved in gpt2_text/checkpoint-616/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1231\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to gpt2_text/checkpoint-770\n",
      "Configuration saved in gpt2_text/checkpoint-770/config.json\n",
      "Configuration saved in gpt2_text/checkpoint-770/generation_config.json\n",
      "Model weights saved in gpt2_text/checkpoint-770/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from gpt2_text/checkpoint-154 (score: 4.003828525543213).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=770, training_loss=3.2265681675502234, metrics={'train_runtime': 234.0681, 'train_samples_per_second': 105.162, 'train_steps_per_second': 3.29, 'total_flos': 401981460480000.0, 'train_loss': 3.2265681675502234, 'epoch': 5.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1231\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/39 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.003828525543213,\n",
       " 'eval_runtime': 1.7459,\n",
       " 'eval_samples_per_second': 705.086,\n",
       " 'eval_steps_per_second': 22.338,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gpt2_text\n",
      "Configuration saved in gpt2_text/config.json\n",
      "Configuration saved in gpt2_text/generation_config.json\n",
      "Model weights saved in gpt2_text/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file gpt2_text/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_length\": 50,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file gpt2_text/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2_text.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file gpt2_text/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = GPT2LMHeadModel.from_pretrained('gpt2_text')\n",
    "device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else 'cpu')\n",
    "finetuned_generator = pipeline(\n",
    "    'text-generation',\n",
    "    model=loaded_model,\n",
    "    tokenizer=tokenizer,\n",
    "    config={'max_new_tokens': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10},\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Women's role in the market should be\n",
      "deduced, by the benevolence, of the community, when she learns that its\n",
      "own customs, institutions, feelings, and habits are the most perfect\n",
      "complementary to her interests.\"\n",
      "\n",
      "----------\n",
      "Women's role in the market should be\n",
      "dedicated solely to the health and happiness of children. The young\n",
      "should always have their education for the benefit of the family;\n",
      "this will always be the result of our benevolent nature, and the bene\n",
      "----------\n",
      "Women's role in the market should never be\n",
      "discriminated against to those ladies who are at the front and bottom, who\n",
      "are very good, but who are deficient in the other branches of their profession.\n",
      "\n",
      "\n",
      "Another interesting thing to\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('-'*10)\n",
    "for generated_sequence in finetuned_generator(\"Women's role in the market should\", num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
