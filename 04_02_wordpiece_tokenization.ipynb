{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30,522'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "f'{len(tokenizer.vocab):,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1037, 3722, 6251, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "text = 'A simple sentence!'\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] a simple sentence! [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2026,\n",
       " 2767,\n",
       " 2409,\n",
       " 2033,\n",
       " 2055,\n",
       " 2023,\n",
       " 2465,\n",
       " 1998,\n",
       " 1045,\n",
       " 2293,\n",
       " 2009,\n",
       " 2061,\n",
       " 2521,\n",
       " 999,\n",
       " 2016,\n",
       " 2001,\n",
       " 2157,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'My friend told me about this class and I love it so far! She was right.'\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('My friend told me about this class and I love it so far! She was right.', 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: 101, subword: [CLS]\n",
      "token: 2026, subword: my\n",
      "token: 2767, subword: friend\n",
      "token: 2409, subword: told\n",
      "token: 2033, subword: me\n",
      "token: 2055, subword: about\n",
      "token: 2023, subword: this\n",
      "token: 2465, subword: class\n",
      "token: 1998, subword: and\n",
      "token: 1045, subword: i\n",
      "token: 2293, subword: love\n",
      "token: 2009, subword: it\n",
      "token: 2061, subword: so\n",
      "token: 2521, subword: far\n",
      "token: 999, subword: !\n",
      "token: 2016, subword: she\n",
      "token: 2001, subword: was\n",
      "token: 2157, subword: right\n",
      "token: 1012, subword: .\n",
      "token: 102, subword: [SEP]\n"
     ]
    }
   ],
   "source": [
    "for t in tokens:\n",
    "    print(f'token: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sunngon' in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7042, 7446, 7459, 1037, 3376, 2154, 102]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_with_unknown_words = 'Sunggon loves a beautiful day'\n",
    "tokens_with_unknown_words = tokenizer.encode(text_with_unknown_words)\n",
    "tokens_with_unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: 101, subword: [CLS]\n",
      "token: 7042, subword: sung\n",
      "token: 7446, subword: ##gon\n",
      "token: 7459, subword: loves\n",
      "token: 1037, subword: a\n",
      "token: 3376, subword: beautiful\n",
      "token: 2154, subword: day\n",
      "token: 102, subword: [SEP]\n"
     ]
    }
   ],
   "source": [
    "for t in tokens_with_unknown_words:\n",
    "    print(f'token: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: 101, subword: [CLS]\n",
      "token: 2175, subword: go\n",
      "token: 2078, subword: ##n\n",
      "token: 102, subword: [SEP]\n"
     ]
    }
   ],
   "source": [
    "for t in tokenizer.encode('gon'):\n",
    "    print(f'token: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: 101, subword: [CLS]\n",
      "token: 7042, subword: sung\n",
      "token: 7446, subword: ##gon\n",
      "token: 2003, subword: is\n",
      "token: 2256, subword: our\n",
      "token: 9450, subword: instructor\n",
      "token: 2005, subword: for\n",
      "token: 2023, subword: this\n",
      "token: 12476, subword: awesome\n",
      "token: 23823, subword: ##sau\n",
      "token: 3401, subword: ##ce\n",
      "token: 2465, subword: class\n",
      "token: 102, subword: [SEP]\n"
     ]
    }
   ],
   "source": [
    "text_with_unknown_words = 'Sunggon is our instructor for this awesomesauce class'\n",
    "tokens_with_unknown_words = tokenizer.encode(text_with_unknown_words)\n",
    "\n",
    "for t in tokens_with_unknown_words:\n",
    "    print(f'token: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 999, 2016, 2001, 2157, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'My friend told me about this class and I love it so far! She was right.'\n",
    "tokens = tokenizer.encode_plus(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 999, 2016, 2001, 2157, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_pet = tokenizer.encode('I love my pet python')\n",
    "python_language = tokenizer.encode('I love coding in python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0626,  0.3430, -0.0584,  ..., -0.1488,  0.3335,  0.5332],\n",
       "         [ 0.2981,  0.3265, -0.1502,  ..., -0.2316,  0.8080,  0.3994],\n",
       "         [ 1.3638,  1.0808,  0.6733,  ..., -0.0417,  0.6030,  0.1758],\n",
       "         ...,\n",
       "         [ 0.4767,  0.2946,  0.8069,  ..., -0.5178,  0.4988,  0.6098],\n",
       "         [-0.1752,  0.1880, -0.6628,  ...,  0.4838,  0.0873, -0.0886],\n",
       "         [ 0.7509,  0.2521, -0.0804,  ...,  0.0018, -0.5243, -0.3450]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.6265e-01, -3.3994e-01, -3.2708e-01,  6.2681e-01,  2.3546e-01,\n",
       "         -7.8475e-02,  8.5182e-01,  2.0823e-01, -2.1635e-01, -9.9996e-01,\n",
       "         -1.3629e-01,  7.3364e-01,  9.8811e-01,  7.5972e-02,  9.2794e-01,\n",
       "         -5.3177e-01, -9.9756e-02, -5.6424e-01,  3.6173e-01, -4.2933e-01,\n",
       "          6.8106e-01,  9.9767e-01,  4.9564e-01,  2.2357e-01,  4.7007e-01,\n",
       "          9.0762e-01, -6.6177e-01,  9.3297e-01,  9.5279e-01,  6.8768e-01,\n",
       "         -5.8403e-01,  1.8561e-01, -9.8950e-01, -1.3498e-01, -3.3305e-01,\n",
       "         -9.9348e-01,  3.5888e-01, -7.6547e-01,  8.2046e-02,  5.0495e-02,\n",
       "         -9.0587e-01,  3.5188e-01,  9.9969e-01, -8.0199e-02,  8.2886e-02,\n",
       "         -2.5628e-01, -1.0000e+00,  1.6943e-01, -8.6229e-01,  3.3145e-01,\n",
       "          2.3511e-01,  1.1805e-01,  1.1162e-01,  4.0433e-01,  4.2662e-01,\n",
       "          2.0258e-01, -1.4351e-01,  1.8008e-02, -1.9211e-01, -5.2570e-01,\n",
       "         -6.5618e-01,  3.7166e-01, -4.7156e-01, -8.8438e-01,  2.6634e-01,\n",
       "          1.1322e-01, -6.9997e-03, -1.7444e-01,  1.1429e-02, -1.4796e-01,\n",
       "          8.5493e-01,  1.5524e-01,  8.9024e-02, -8.2204e-01, -8.8158e-02,\n",
       "          1.4236e-01, -5.5706e-01,  1.0000e+00, -2.0625e-01, -9.8055e-01,\n",
       "          4.0588e-01,  3.7352e-02,  4.5972e-01,  4.4094e-01, -1.1954e-01,\n",
       "         -1.0000e+00,  3.4915e-01, -9.4497e-02, -9.9087e-01,  2.4873e-01,\n",
       "          5.1711e-01, -1.5861e-01,  1.0393e-01,  4.8994e-01, -3.1361e-01,\n",
       "         -2.4621e-01, -2.2043e-01, -3.1757e-01, -1.9226e-01, -9.4250e-02,\n",
       "          3.3021e-02, -1.6159e-01, -9.3973e-02, -3.3711e-01,  1.4852e-01,\n",
       "         -4.0279e-01, -4.9204e-01,  2.8474e-01, -2.3164e-01,  6.4485e-01,\n",
       "          2.8742e-01, -2.5793e-01,  3.3261e-01, -9.5431e-01,  6.2212e-01,\n",
       "         -2.3859e-01, -9.8475e-01, -4.7376e-01, -9.9114e-01,  7.1689e-01,\n",
       "         -3.0232e-02, -1.3477e-01,  9.6188e-01,  3.3964e-01,  3.1937e-01,\n",
       "          3.5653e-02, -3.5928e-01, -1.0000e+00, -3.3136e-01, -7.9348e-02,\n",
       "          1.5473e-01, -1.6041e-01, -9.8125e-01, -9.5617e-01,  5.5156e-01,\n",
       "          9.4402e-01,  8.8326e-02,  9.9948e-01, -1.9225e-01,  9.4637e-01,\n",
       "          7.5049e-02, -2.8451e-01,  5.3652e-02, -4.4577e-01,  3.8773e-01,\n",
       "          2.1066e-01, -6.8323e-01,  1.8714e-01, -1.7723e-02,  1.0850e-01,\n",
       "         -2.9349e-01, -1.6138e-01, -1.0996e-01, -9.3950e-01, -3.6886e-01,\n",
       "          9.4668e-01, -4.1515e-02, -3.5104e-01,  5.2211e-01, -1.0684e-01,\n",
       "         -3.5152e-01,  8.4455e-01,  3.2870e-01,  3.1623e-01, -1.4047e-01,\n",
       "          3.4095e-01, -5.3571e-02,  5.3367e-01, -8.0936e-01,  2.4783e-01,\n",
       "          3.8211e-01, -2.2251e-01, -1.7682e-01, -9.8147e-01, -3.1191e-01,\n",
       "          5.0458e-01,  9.8934e-01,  7.7420e-01,  2.3857e-01,  4.2886e-01,\n",
       "         -1.5355e-01,  4.7658e-01, -9.5124e-01,  9.8250e-01, -1.8514e-01,\n",
       "          2.3269e-01, -1.0956e-01,  5.0999e-02, -8.4263e-01, -1.2775e-01,\n",
       "          7.5939e-01, -4.8416e-01, -8.4385e-01,  6.2579e-02, -4.4718e-01,\n",
       "         -3.2046e-01, -4.1981e-01,  4.3337e-01, -2.3733e-01, -3.3002e-01,\n",
       "         -6.5690e-02,  9.2735e-01,  9.4813e-01,  7.8466e-01, -3.3182e-01,\n",
       "          4.4505e-01, -9.1821e-01, -4.2330e-01,  6.3979e-02,  2.1344e-01,\n",
       "          1.4185e-01,  9.9336e-01, -2.9228e-01, -4.7931e-02, -9.1447e-01,\n",
       "         -9.8644e-01, -5.1444e-02, -8.6226e-01, -4.3437e-02, -6.5530e-01,\n",
       "          4.4678e-01,  1.7737e-01, -2.4307e-02,  3.4202e-01, -9.7717e-01,\n",
       "         -7.1580e-01,  3.5369e-01, -3.2609e-01,  3.6993e-01, -2.3212e-01,\n",
       "          8.1618e-01,  5.2785e-01, -5.9748e-01,  6.4338e-01,  8.7944e-01,\n",
       "         -3.3572e-01, -6.7549e-01,  7.1383e-01, -1.7536e-01,  8.4860e-01,\n",
       "         -6.0251e-01,  9.8558e-01,  4.5635e-01,  5.9987e-01, -9.4081e-01,\n",
       "         -1.2301e-01, -8.7137e-01, -4.8865e-02, -5.7984e-02, -3.5732e-01,\n",
       "          3.9556e-01,  4.7796e-01,  3.5414e-01,  7.0683e-01, -5.1474e-01,\n",
       "          9.9182e-01, -8.6538e-01, -9.5231e-01, -4.4330e-01, -2.0626e-01,\n",
       "         -9.8958e-01,  4.6572e-01,  2.7898e-01, -2.1362e-01, -3.9663e-01,\n",
       "         -5.2700e-01, -9.6002e-01,  8.6310e-01,  5.8796e-02,  9.8029e-01,\n",
       "         -1.1540e-01, -8.9042e-01, -3.7505e-01, -9.3668e-01, -1.8700e-01,\n",
       "         -1.1768e-01,  2.6991e-01, -2.1028e-01, -9.5448e-01,  4.2051e-01,\n",
       "          4.9113e-01,  4.3175e-01, -6.6314e-02,  9.9514e-01,  9.9998e-01,\n",
       "          9.8130e-01,  8.7488e-01,  8.8306e-01, -9.9231e-01, -4.6141e-01,\n",
       "          9.9998e-01, -9.1483e-01, -1.0000e+00, -9.2109e-01, -5.2632e-01,\n",
       "          2.7114e-01, -1.0000e+00, -1.7468e-02,  1.1033e-01, -9.3706e-01,\n",
       "         -8.1485e-02,  9.8030e-01,  9.7809e-01, -1.0000e+00,  8.5570e-01,\n",
       "          9.3970e-01, -5.0361e-01,  7.7610e-01, -2.1006e-01,  9.7798e-01,\n",
       "          3.8062e-01,  3.7450e-01, -1.6165e-01,  3.8503e-01, -6.4965e-01,\n",
       "         -8.3592e-01,  1.3620e-01, -2.0421e-01,  9.4375e-01,  1.0990e-01,\n",
       "         -5.7771e-01, -9.3801e-01,  1.6080e-01, -7.8276e-02, -2.3798e-01,\n",
       "         -9.6181e-01, -1.5998e-01, -9.0215e-04,  7.1215e-01,  7.0243e-02,\n",
       "          2.0467e-01, -7.0379e-01,  1.9083e-01, -6.1515e-01,  3.8001e-01,\n",
       "          5.9697e-01, -9.4315e-01, -5.9893e-01,  1.0099e-01, -4.0772e-01,\n",
       "         -5.2215e-02, -9.5575e-01,  9.6943e-01, -2.9231e-01,  3.5944e-01,\n",
       "          1.0000e+00,  1.2807e-01, -8.7589e-01,  3.4739e-01,  2.2912e-01,\n",
       "         -3.5046e-01,  1.0000e+00,  6.1486e-01, -9.8036e-01, -4.4399e-01,\n",
       "          4.0402e-01, -4.6335e-01, -4.4879e-01,  9.9901e-01, -2.3758e-01,\n",
       "          3.4463e-02,  2.8088e-01,  9.8146e-01, -9.9083e-01,  9.0611e-01,\n",
       "         -8.9091e-01, -9.6832e-01,  9.6013e-01,  9.3312e-01, -2.4725e-01,\n",
       "         -5.6586e-01, -3.2778e-02, -1.7659e-01,  2.2424e-01, -9.4092e-01,\n",
       "          5.7691e-01,  3.8251e-01, -1.0246e-01,  8.8165e-01, -7.4959e-01,\n",
       "         -4.8928e-01,  2.8635e-01,  1.3415e-01,  2.5645e-01,  5.2807e-01,\n",
       "          3.9654e-01, -1.8426e-01,  8.6351e-02, -2.1589e-01, -5.3565e-01,\n",
       "         -9.7048e-01,  1.6575e-01,  1.0000e+00, -1.0645e-01,  2.5626e-01,\n",
       "         -2.5886e-01,  4.3988e-02, -2.6555e-01,  4.4175e-01,  4.9805e-01,\n",
       "         -2.9459e-01, -8.3595e-01,  5.0811e-01, -9.2918e-01, -9.8975e-01,\n",
       "          6.5703e-01,  1.4751e-01, -2.4364e-01,  9.9986e-01,  3.0631e-01,\n",
       "          2.1495e-01,  1.3191e-01,  8.1215e-01, -1.4006e-01,  4.5342e-01,\n",
       "          1.8044e-01,  9.7521e-01, -2.1461e-01,  4.2920e-01,  7.6196e-01,\n",
       "         -2.8865e-01, -2.2229e-01, -6.3327e-01,  6.6843e-02, -9.1075e-01,\n",
       "          1.7250e-01, -9.5649e-01,  9.6108e-01,  3.9162e-01,  2.6024e-01,\n",
       "          8.6417e-02,  1.2142e-01,  1.0000e+00, -5.9195e-01,  5.5917e-01,\n",
       "         -1.9394e-01,  7.5159e-01, -9.9137e-01, -7.5230e-01, -3.9367e-01,\n",
       "         -1.5384e-02, -1.8796e-01, -2.9521e-01,  1.8371e-01, -9.7314e-01,\n",
       "          1.0733e-01,  2.3842e-01, -9.7080e-01, -9.9125e-01,  1.1150e-01,\n",
       "          6.3250e-01,  6.6505e-02, -8.4048e-01, -6.5465e-01, -5.9462e-01,\n",
       "          3.5661e-01, -1.8127e-01, -9.3799e-01,  4.5990e-01, -1.5086e-01,\n",
       "          3.9046e-01, -8.8860e-02,  4.9600e-01,  1.6321e-01,  7.9235e-01,\n",
       "          1.9128e-01,  6.1970e-02, -2.2908e-03, -7.4228e-01,  7.5543e-01,\n",
       "         -7.5471e-01, -4.1221e-01, -1.0795e-01,  1.0000e+00, -3.3480e-01,\n",
       "          3.9278e-01,  6.5489e-01,  6.0476e-01, -1.2312e-01,  1.9247e-01,\n",
       "          4.7518e-01,  2.3333e-01, -1.5425e-01, -3.6385e-02, -4.3970e-01,\n",
       "         -2.7742e-01,  4.9623e-01, -4.6630e-02,  4.6329e-02,  7.9993e-01,\n",
       "          5.0040e-01,  1.6812e-01,  1.0671e-01, -5.0505e-02,  9.9793e-01,\n",
       "         -1.2588e-01, -2.0244e-01, -4.1670e-01,  4.2333e-02, -3.1529e-01,\n",
       "         -2.3717e-01,  1.0000e+00,  2.6738e-01, -3.4986e-02, -9.9246e-01,\n",
       "         -2.8613e-01, -8.9860e-01,  9.9995e-01,  8.2426e-01, -7.8457e-01,\n",
       "          4.9651e-01,  4.7128e-01, -9.5581e-02,  7.3666e-01, -1.4170e-01,\n",
       "         -1.7861e-01,  1.6581e-01, -1.2957e-02,  9.7139e-01, -4.5294e-01,\n",
       "         -9.7705e-01, -5.5786e-01,  3.4721e-01, -9.6677e-01,  9.9468e-01,\n",
       "         -4.6806e-01, -2.4942e-01, -4.2260e-01,  8.7309e-02,  1.9633e-01,\n",
       "         -6.3328e-02, -9.8258e-01, -4.9296e-02,  1.0526e-01,  9.7270e-01,\n",
       "          1.4295e-01, -4.7255e-01, -8.7287e-01,  6.6273e-02,  1.2849e-01,\n",
       "         -3.1866e-01, -9.4873e-01,  9.7686e-01, -9.7782e-01,  5.3707e-01,\n",
       "          1.0000e+00,  2.7873e-01, -5.4250e-01,  2.3226e-01, -3.8389e-01,\n",
       "          2.3646e-01, -3.1635e-02,  4.6457e-01, -9.5191e-01, -3.1576e-01,\n",
       "         -1.4624e-01,  2.2123e-01, -8.6176e-02,  4.7302e-02,  6.5857e-01,\n",
       "          1.7958e-01, -4.4928e-01, -5.7624e-01, -5.3406e-02,  3.2073e-01,\n",
       "          7.5750e-01, -1.9905e-01, -7.9276e-02, -2.8727e-02, -3.4747e-02,\n",
       "         -9.2644e-01, -2.8820e-01, -2.5835e-01, -9.9936e-01,  5.5512e-01,\n",
       "         -1.0000e+00, -1.2652e-01, -3.5275e-01, -1.5816e-01,  7.8472e-01,\n",
       "          4.5758e-01,  2.2100e-01, -7.0314e-01, -1.3694e-01,  7.8445e-01,\n",
       "          7.3643e-01, -1.6913e-01,  4.3172e-02, -7.1202e-01,  1.9336e-01,\n",
       "         -1.0635e-01,  3.0037e-01,  3.0777e-02,  7.1302e-01, -1.1352e-01,\n",
       "          1.0000e+00,  4.3366e-02, -3.9448e-01, -9.6177e-01,  2.1395e-01,\n",
       "         -1.5061e-01,  1.0000e+00, -8.0856e-01, -9.5191e-01,  3.6001e-01,\n",
       "         -5.3518e-01, -7.9884e-01,  1.7865e-01, -1.0266e-01, -6.8854e-01,\n",
       "         -6.3346e-01,  9.3626e-01,  7.2052e-01, -4.5045e-01,  3.9521e-01,\n",
       "         -2.4737e-01, -4.2217e-01, -1.1776e-01,  2.7317e-01,  9.8948e-01,\n",
       "          3.3028e-01,  8.7008e-01,  1.1375e-01, -1.3805e-01,  9.6214e-01,\n",
       "          1.8889e-01,  3.7109e-01,  1.0364e-01,  1.0000e+00,  2.4269e-01,\n",
       "         -8.9554e-01,  1.4313e-01, -9.7642e-01, -1.7584e-01, -9.2687e-01,\n",
       "          2.0433e-01,  3.8210e-02,  8.7873e-01, -1.7118e-01,  9.5909e-01,\n",
       "         -5.2010e-02, -2.7752e-02, -1.2294e-01,  3.3254e-01,  3.1784e-01,\n",
       "         -9.2368e-01, -9.8899e-01, -9.8728e-01,  4.4558e-01, -4.0266e-01,\n",
       "          1.2097e-01,  2.2264e-01, -2.4829e-03,  3.2007e-01,  3.8498e-01,\n",
       "         -1.0000e+00,  9.2933e-01,  3.7576e-01,  3.7009e-01,  9.6858e-01,\n",
       "          5.4077e-01,  3.9828e-01,  2.6489e-01, -9.8669e-01, -9.6060e-01,\n",
       "         -3.3861e-01, -1.4512e-01,  7.3075e-01,  5.8343e-01,  8.3563e-01,\n",
       "          3.8583e-01, -5.0754e-01, -3.7464e-01,  1.8943e-01, -7.9924e-01,\n",
       "         -9.9316e-01,  3.7975e-01,  1.4004e-01, -9.2670e-01,  9.5007e-01,\n",
       "         -4.6589e-01, -9.6339e-02,  4.0248e-01, -2.5874e-01,  9.2399e-01,\n",
       "          7.4748e-01,  4.2015e-01,  1.4271e-01,  4.5074e-01,  8.8692e-01,\n",
       "          9.2110e-01,  9.8998e-01, -3.2624e-01,  7.0185e-01, -1.5632e-01,\n",
       "          3.6682e-01,  7.8638e-01, -9.3758e-01,  4.6584e-02,  9.0874e-02,\n",
       "         -1.6201e-01,  2.2313e-01, -2.1142e-01, -9.4973e-01,  3.7510e-01,\n",
       "         -2.1983e-01,  5.1252e-01, -3.6013e-01,  1.2994e-01, -4.4232e-01,\n",
       "         -1.5051e-01, -6.5631e-01, -5.2294e-01,  5.8929e-01,  2.6858e-01,\n",
       "          9.1114e-01,  6.6467e-01, -6.5823e-02, -6.2090e-01, -1.6422e-01,\n",
       "         -1.1032e-01, -9.3132e-01,  9.0382e-01, -2.3122e-02,  2.0194e-01,\n",
       "          1.8310e-02, -1.5393e-01,  8.3186e-01, -2.5173e-01, -3.4774e-01,\n",
       "         -2.8389e-01, -7.1799e-01,  8.5714e-01, -2.8193e-01, -4.8119e-01,\n",
       "         -4.8052e-01,  7.2177e-01,  3.0301e-01,  9.9865e-01, -2.2744e-01,\n",
       "         -4.6294e-01, -1.7313e-01, -2.9101e-01,  3.3487e-01, -3.3648e-01,\n",
       "         -1.0000e+00,  3.2309e-01, -1.2202e-02,  2.6391e-01, -3.0067e-01,\n",
       "          3.5337e-01, -6.9408e-02, -9.6779e-01, -2.1858e-01,  3.0723e-01,\n",
       "          1.3199e-01, -5.2009e-01, -3.4677e-01,  4.8403e-01,  1.9928e-01,\n",
       "          7.6263e-01,  8.5758e-01,  4.0402e-01,  5.6715e-01,  5.3558e-01,\n",
       "         -3.4730e-02, -6.4096e-01,  9.0126e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(python_pet).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(python_pet).unsqueeze(0))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(python_pet).unsqueeze(0))[0][:, 5, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_pet_embedding = model(torch.tensor(python_pet).unsqueeze(0))[0][:, 5, :].detach().numpy() # python is 6th token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_language_embedding = model(torch.tensor(python_language).unsqueeze(0))[0][:, 5, :].detach().numpy() # python is 6th token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake_alone_embedding = model(torch.tensor(tokenizer.encode('snake')).unsqueeze(0))[0][:, 1, :].detach().numpy()\n",
    "programming_alone_embedding = model(torch.tensor(tokenizer.encode('programming')).unsqueeze(0))[0][:, 1, :].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.6928656]], dtype=float32), array([[0.49864388]], dtype=float32))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(python_pet_embedding, snake_alone_embedding), cosine_similarity(python_pet_embedding, programming_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.]], dtype=float32), array([[0.6274053]], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(programming_alone_embedding, programming_alone_embedding), cosine_similarity(programming_alone_embedding, snake_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49864388]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(programming_alone_embedding, python_pet_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
