{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "base_tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text preprocessed:\n",
      " Sinan Ozdemir is a data scientist, startup founder, and educator living in the San Francisco Bay Area with his dog, Charlie; cat, Euclid; and bearded dragon, Fiero. He spent his academic career studying pure mathematics at Johns Hopkins University before transitioning to education. He spent several years conducting lectures on data science at Johns Hopkins University and at the General Assembly before founding his own start-up,Legion Analytics, which uses artificial intelligence and data science to power enterprise sales teams. After completing the Fellowship at the Y Combinator accelerator, Sinan has spent most of his days working on his fast-growing company, while creating educational material for data science.\n"
     ]
    }
   ],
   "source": [
    "text_to_summarize = '''Sinan Ozdemir is a data scientist, startup founder, and educator living in the San Francisco Bay Area with his dog, \n",
    "Charlie; cat, Euclid; and bearded dragon, Fiero. He spent his academic career studying pure mathematics \n",
    "at Johns Hopkins University before transitioning to education. He spent several years conducting lectures \n",
    "on data science at Johns Hopkins University and at the General Assembly before founding his own start-up,\n",
    "Legion Analytics, which uses artificial intelligence and data science to power enterprise sales teams. \n",
    "After completing the Fellowship at the Y Combinator accelerator, Sinan has spent most of his days working on \n",
    "his fast-growing company, while creating educational material for data science.\n",
    "'''\n",
    "preprocess_text = text_to_summarize.strip().replace('\\n', '')\n",
    "print('original text preprocessed:\\n', preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized text: \n",
      "Sinan Ozdemir is a data scientist, startup founder, and educator. he founded his own start-up, which uses artificial intelligence and data science to power sales teams.\n"
     ]
    }
   ],
   "source": [
    "t5_prepared_text = 'summarize: ' + preprocess_text\n",
    "input_ids = base_tokenizer.encode(t5_prepared_text, return_tensors='pt')\n",
    "summary_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    min_length=30,\n",
    "    max_length=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(f'summarized text: \\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tranlated text:\n",
      "Wo ist die Schokolade?\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode('translate English to German: Where is the chocolate?', return_tensors='pt')\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "print(f'Tranlated text:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3488,   229,    67, 31267,    58,     1]]),\n",
       " tensor(0.1136, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = base_tokenizer('translate English to German: Where is the chocolate?', return_tensors='pt').input_ids\n",
    "labels = base_tokenizer('Wo ist die Schokolade?', return_tensors='pt').input_ids\n",
    "loss = base_model(input_ids=input_ids, labels=labels).loss\n",
    "labels, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is grammatically correct?: \n",
      "acceptable\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode('cola sentence: Where is the chocolate?', return_tensors='pt')\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "print(f'is grammatically correct?: \\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is grammatically correct?: \n",
      "unacceptable\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode('cola sentence: Where be a chocolate?', return_tensors='pt')\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "print(f'is grammatically correct?: \\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is semantically similar? (0-5): \n",
      "3.2\n"
     ]
    }
   ],
   "source": [
    "sentence_one = 'How to fish'\n",
    "sentence_two = 'Fishing Manual for beginners'\n",
    "input_ids = base_tokenizer.encode(f'stsb sentence1: {sentence_one} sentence2: {sentence_two}', return_tensors='pt')\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=3,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "print(f'is semantically similar? (0-5): \\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is semantically similar? (0-5): \n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "sentence_one = 'How to fish'\n",
    "sentence_two = 'Hiking Manual for beginners'\n",
    "input_ids = base_tokenizer.encode(f'stsb sentence1: {sentence_one} sentence2: {sentence_two}', return_tensors='pt')\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=3,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "print(f'is semantically similar? (0-5): \\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "entailment\n"
     ]
    }
   ],
   "source": [
    "# entailment, contradiction, netural\n",
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I am running for mayor', return_tensors='pt'\n",
    ")\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "print(f'Response: \\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "contradiction\n"
     ]
    }
   ],
   "source": [
    "# entailment, contradiction, netural\n",
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I do not really vote', return_tensors='pt'\n",
    ")\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "print(f'Response: \\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "# entailment, contradiction, netural\n",
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I code for a living', return_tensors='pt'\n",
    ")\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "print(f'Response: \\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "Saitama, Japan\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'question: Where does Sunggon live? context: Sunggon lives in Saitama, Japan but Sigyo lives in Seoul, Korea', \n",
    "    return_tensors='pt'\n",
    ")\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "print(f'Response: \\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "Seoul, Korea\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'question: Where does Sigyo live? context: Sunggon lives in Saitama, Japan but Sigyo lives in Seoul, Korea',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "print(f'Response: \\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "not_duplicate\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'prompt1: Where does Sunggon live? prompt2: Sunggon lives in Saitama, Japan but Sigyo lives in Seoul, Korea', \n",
    "    return_tensors='pt'\n",
    ")\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "print(f'Response: \\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
